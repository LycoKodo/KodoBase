{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project bases off the video tutorial by Andrej Karpathy, inspired by Vedal from Neuro-sama\n",
    "\n",
    "    Source video - https://www.youtube.com/watch?v=kCc8FmEb1nY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be training this system on shakespeare's writing (tinyshakespeare), to generate \"infinite\" shakespeare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training Data (Shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in chars:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"tiny-shake.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#inspecting dataset\n",
    "\n",
    "print(\"length of dataset in chars: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text))) #create a list of set of characters in text\n",
    "vocab_size = len(chars) # Length of vocabulary\n",
    "\n",
    "# Getting a preview of vocab (rly just all possible chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Token Encoder & Decoder (char level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO How tf does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hiii there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) } # creating lookup table on chars\n",
    "itos = { i:ch for  i,ch in enumerate(chars) } # decoder in chars\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] #takes string and output ints\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) #takes ints to string\n",
    "\n",
    "print(encode(\"hiii there\"))\n",
    "print(decode(encode(\"hiii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are encoding single chars as our tokens (simple to implement, but longer output token), openAI and Google often uses tokenisation like \"Subword\" and \"Tiktoken\" which encodes tokens from word segments. \n",
    "\n",
    "TODO: how does this improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding shakespeare into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Encoding entire text file into a tensor of char tokens\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train and val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # 90% is train, 10 is test\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT - block size\n",
    "\n",
    "most transformers train by blocks of text, and block size is the size of each block of text which is fed into the transformer during training.\n",
    "\n",
    "TODO - Is this like batch size? \n",
    "       Why does it improve performance?\n",
    "       Does increasing or decreasing block size affect performance? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With line:\n",
    "    tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
    "\n",
    "(limited udnerstanding, need more research) We see the places where the model will have to predict from its current number the next number. \n",
    "\n",
    "The code below explains this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), the target is 47\n",
      "when input is tensor([18, 47]), the target is 56\n",
      "when input is tensor([18, 47, 56]), the target is 57\n",
      "when input is tensor([18, 47, 56, 57]), the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will try to predict the target, but it will never recieve inputs exceeding input of block size.\n",
    "\n",
    "TODO - is this the context window?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also seems that tehse chunks or batches, can optimise training by leverging off gpu parallel processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Data Batches of Chunks of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "----------------------\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------------------\n",
      "When input is [24], target is 43\n",
      "When input is [24, 43], target is 58\n",
      "When input is [24, 43, 58], target is 5\n",
      "When input is [24, 43, 58, 5], target is 57\n",
      "When input is [24, 43, 58, 5, 57], target is 1\n",
      "When input is [24, 43, 58, 5, 57, 1], target is 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], target is 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], target is 39\n",
      "When input is [44], target is 53\n",
      "When input is [44, 53], target is 56\n",
      "When input is [44, 53, 56], target is 1\n",
      "When input is [44, 53, 56, 1], target is 58\n",
      "When input is [44, 53, 56, 1, 58], target is 46\n",
      "When input is [44, 53, 56, 1, 58, 46], target is 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], target is 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], target is 1\n",
      "When input is [52], target is 58\n",
      "When input is [52, 58], target is 1\n",
      "When input is [52, 58, 1], target is 58\n",
      "When input is [52, 58, 1, 58], target is 46\n",
      "When input is [52, 58, 1, 58, 46], target is 39\n",
      "When input is [52, 58, 1, 58, 46, 39], target is 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], target is 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], target is 46\n",
      "When input is [25], target is 17\n",
      "When input is [25, 17], target is 27\n",
      "When input is [25, 17, 27], target is 10\n",
      "When input is [25, 17, 27, 10], target is 0\n",
      "When input is [25, 17, 27, 10, 0], target is 21\n",
      "When input is [25, 17, 27, 10, 0, 21], target is 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], target is 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], target is 39\n"
     ]
    }
   ],
   "source": [
    "# Creating Chunks in the dataset that are going to be inferenced independently\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generating random locations to take batches\n",
    "    \n",
    "    # stacking x and y vertically\n",
    "        # [][]\n",
    "        # to\n",
    "        # []\n",
    "        # []\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb  = get_batch('train') #getting batches of the test data set\n",
    "\n",
    "# xb is to be fed into transformer\n",
    "# yb is to be supposed output\n",
    "\n",
    "print('inputs: ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print(\"----------------------\")\n",
    "\n",
    "print('Targets: ')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----------------------\")\n",
    "\n",
    "for b in range(batch_size): #batch dimension\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1] #context (input)\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()}, target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Into Neural Net (Bigram Language Model) Needs Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram means 2 words! This is an early form of NLP (Natural Language Processing) called N-gram language model that generates text based on tokens of 2 words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn #neural network\n",
    "from torch.nn import functional as f\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# subclass of nn module\n",
    "class BigrameLanguageModel(nn.Module):\n",
    "\n",
    "    # python's constructor\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "            # TODO I absolutely do not understand this comment\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        #idx and targets are both (B, T) tensors of ints\n",
    "            # Batch, Time, Channels\n",
    "            # 4, 8, Vocab_size (65) \n",
    "\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        # loss = f.cross_entropy(logits, targets) \n",
    "        # it seems that pytorch actually wants (B, C, T)\n",
    "            # So we need to reshape logits\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #calculating loss\n",
    "\n",
    "            logits = logits.view (B*T, C) # linear alg math here ig\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = f.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss #scores for next characters????\n",
    "    \n",
    "    # So now with this model, we can evaluate loss, now lets generate\n",
    "        # TODO WHAT IS GOING ON\n",
    "\n",
    "    # so apparantly continueing generation of future tokens is looped until max tokens and appended to response\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B, T) array of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            #getting predictions\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            #apply softmax to get probabilities\n",
    "            probs = f.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "            #sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            #append sampled index to the running seq\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = BigrameLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # Initial loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating From model (untrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pJ:Bpm&yiltNCjeO3:Cx&vvMYW-txjuAd IRFbTpJ$zkZelxZtTlHNzdXXUiQQY:qFINTOBNLI,&oTigq z.c:Cq,SDXzetn3XVj\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long) #feeding in as first character/prompt\n",
    "\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a PyTorch optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5006003379821777\n",
      "2.3401167392730713\n",
      "2.3401167392730713\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "presc = loss.item()\n",
    "\n",
    "while presc > 2.4:\n",
    "    for steps in range(2000):\n",
    "\n",
    "        #sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        #evaluate the loss\n",
    "        logits, loss = m(xb, yb)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    presc = loss.item()\n",
    "    print(loss.item())\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prlltthere w;\n",
      "DURURIZAUCERosthoras be hemay gsonsigre gh ld y ho Fowavereese r\n",
      "AD o err uee pollluivera Wig INut\n",
      "\n",
      "ST:\n",
      "Broumathaf it fa pul!\n",
      "Ag chowit armo,\n",
      "An l, bempeveathrrg d othat,\n",
      "\n",
      "Grdmat us ce as haspa? aporse t itlve\n",
      "Mas read pr borews ath\n",
      "Ifo wand ve tothelcheru be aghe, blbu whes,\n",
      "Wambr; th\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
